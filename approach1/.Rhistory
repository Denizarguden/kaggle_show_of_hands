rm(list=ls())
getwd()
setwd('C:/Users/Desk 1/Desktop/kaggle_show_of_hands/approach1')
setwd('/Users/hawooksong/Desktop/kaggle_show_of_hands')
setwd('/Users/hawooksong/Desktop/kaggle_show_of_hands/approach1')
dir()
train_test.data.file <- 'data/raw/train_test.csv'
final_test.data.file <- 'data/raw/final_test.csv'
train_test.original <- read.csv(train_test.data.file,
colClasses=train_test.column.types,
na.strings=missing.types)
train_test.column.types <- c('integer',   # UserID
'integer',   # YOB
'factor',    # Gender
'factor',    # Income
'factor',    # HouseholdStatus
'factor',    # EducationLevel
'factor',    # Party
'integer',   # Happy
rep('factor', 101),    # questions
'integer')       # votes
final_test.column.types <- train_test.column.types[-8]
train_test.original <- read.csv(train_test.data.file,
colClasses=train_test.column.types,
na.strings=missing.types)
missing.types <- c("NA", "")
train_test.original <- read.csv(train_test.data.file,
colClasses=train_test.column.types,
na.strings=missing.types)
head(train_test.original)
head(train_test.original)
library(Amelia)
missmap(train_test.original, main="Show of Hands Training/Testing Data - Missings Map",
col=c("yellow", "black"), legend=FALSE)
missmap(train_test.original, main="Show of Hands Training/Testing Data - Missings Map",
col=c("yellow", "black"), legend=FALSE)
missmap(train_test.original, main="Show of Hands Initial Training/Testing Data - Missing Data Map",
col=c("yellow", "black"), legend=FALSE)
dir()
dir()
dir('..')
dir('../figures/missingDataMap.png')
dev.copy(png, '../figures/missingDataMap.png')
dev.off()
missmap(train_test.original, main="Show of Hands Initial Training/Testing Data - Missing Data Map",
col=c("yellow", "black"), legend=FALSE)
missmap(train_test.original, main="Show of Hands Initial Training/Testing Data \n Missing Data Map",
col=c("yellow", "black"), legend=FALSE)
dev.copy(png, '../figures/missingDataMap.png')
dev.off()
train_test.original$Age <- 2014 - train_test.original$YOB
final_test.original$Age <- 2014 - final_test.original$YOB
final_test.original <- read.csv(final_test.data.file,
colClasses=final_test.column.types,
na.strings=missing.types)
train_test.original$Age <- 2014 - train_test.original$YOB
final_test.original$Age <- 2014 - final_test.original$YOB
ggplot(train_test.original) +
geom_histogram(aes(x=Age), binwidth=1)
library(ggplot2)
ggplot(train_test.original) +
geom_histogram(aes(x=Age), binwidth=1)
head(sort(train_test.original$Age), 40)
dir()
setwd('/Users/hawooksong/Desktop/kaggle_show_of_hands/approach1')
dir()
train_test <- train_test.imputed
train_test <- read.csv('data/imputed/train_test_imputed.csv')
final_test <- read.csv('data/imputed/final_test_imputed.csv')
str(train_test)
head(train_test)
hist(train_test$Age)
hist(train_test$votes)
library(psych)
describe(train_test$Age)
describe(train_test$votes)
library(psych)
describe(train_test$Age)
describe(train_test$votes)
library(caTools)
set.seed(123)
split <- sample.split(train_test$Happy, SplitRatio=0.75)
train <- train_test[split==TRUE, ]
test <- train_test[split==FALSE, ]
nrow(train)
nrow(test)
table(train$Happy)
1660 / (1286 + 1660)  # 56.35% accuracy on train
table(test$Happy)
553 / (428 + 553)  # 56.37% accuracy on test
table(test$Happy)
553 / (428 + 553)  # 56.37% accuracy on test
logModel <- glm(Happy ~ ., data = train, family = binomial)
summary(logModel)
predPerc.logModel.train <- predict(logModel, newdata = train, type = 'response')
head(predPerc.logModel.train)
predPerc.logModel.test <- predict(logModel, newdata = test, type = 'response')
head(predPerc.logModel.test)
pred.logModel.test <- ifelse(predPerc.logModel.test > 0.5, 1, 0)
table(pred.logModel.test, test$Happy)
logModel <- glm(Happy ~ . - UserID - YOB, data = train, family = binomial)
head(train)
train_test.imputed <- train_test
head(train_test)
write.csv(train_test.imputed,
file='data/imputed/train_test_imputed.csv',
row.names = FALSE)
write.csv(final_test.imputed,
file='data/imputed/final_test_imputed.csv',
row.names = FALSE)
final_test.imputed <- final_test
head(final_test)
write.csv(final_test.imputed,
file='data/imputed/final_test_imputed.csv',
row.names = FALSE)
rm()
rm(list = ls())
setwd('/Users/hawooksong/Desktop/kaggle_show_of_hands/approach1')
train_test <- read.csv('data/imputed/train_test_imputed.csv')
final_test <- read.csv('data/imputed/final_test_imputed.csv')
library(caTools)
set.seed(123)
split <- sample.split(train_test$Happy, SplitRatio=0.75)
train <- train_test[split==TRUE, ]
test <- train_test[split==FALSE, ]
head(train)
library(caTools)
set.seed(123)
split <- sample.split(train_test$Happy, SplitRatio=0.75)
train <- train_test[split==TRUE, ]
test <- train_test[split==FALSE, ]
head(train)
names(train)
?write.csv
head(train_test.imputed)
head(train_test)
train_test.imputed <- train_test
train_test.imputed <- subset(train_test.imputed, select = -X)
write.csv(train_test.imputed,
file='data/imputed/train_test_imputed.csv',
row.names = FALSE)
final_test.imputed <- subset(final_test.imputed, select = - X)
final_test.imputed <- final_test
final_test.imputed <- subset(final_test.imputed, select = - X)
write.csv(train_test.imputed,
file='data/imputed/train_test_imputed.csv',
row.names = FALSE)
write.csv(final_test.imputed,
file='data/imputed/final_test_imputed.csv',
row.names = FALSE)
train_test <- read.csv('data/imputed/train_test_imputed.csv')
final_test <- read.csv('data/imputed/final_test_imputed.csv')
head(train)
set.seed(123)
split <- sample.split(train_test$Happy, SplitRatio=0.75)
train <- train_test[split==TRUE, ]
test <- train_test[split==FALSE, ]
head(train)
logModel <- glm(Happy ~ . - UserID - YOB, data = train, family = binomial)
logModel <- glm(Happy ~ . - YOB, data = train, family = binomial)
predPerc.logModel.train <- predict(logModel, newdata = train, type = 'response')
pred.logModel.train <- ifelse(predPerc.logModel.train > 0.5, 1, 0)
table(pred.logModel.train, train$Happy)
library(rpart)  # for rpart()
library(rpart.plot)  # for prp()
CARTmodel <- rpart(Happy ~ ., data = train, method = 'class')
prp(CARTmodel, varlen=0)  # varlen=0 instructs prp() to use full variable names
# CARTmodel prediction percentages on train
predPerc.CARTmodel.train <- predict(CARTmodel)[ , 2]
head(predPerc.CARTmodel.train)
# picking the best cutoff percentage value for CARTmodel on train
plotErrorsByCutoff(predPerc.CARTmodel.train, train$Happy)
# best cutoff value: between 0.4 and 0.5
# calculating the accuracy of CARTmodel on train
pred.CARTmodel.train <- ifelse(predPerc.CARTmodel.train > 0.45, 1, 0)
table(pred.CARTmodel.train, train$Happy)
plotErrorsByCutoff <- function(predPerc, observation) {
cutoff <- (1:10) * 0.1;
errorsByCutoff <- rep(NA, 10);
for (i in 1:length(cutoff)) {
predicted <- as.integer(predPerc > cutoff[i])
nErrors <- sum(predicted != observation)
errorsByCutoff[i] <- nErrors
};
plot(cutoff, errorsByCutoff, xlab='Cutoff', ylab='Error count',
main='Number of prediction errors\n under various cutoff values', pch=20);
}
logModel <- glm(Happy ~ . - YOB, data = train, family = binomial)
predPerc.logModel.train <- predict(logModel, newdata = train, type = 'response')
pred.logModel.train <- ifelse(predPerc.logModel.train > 0.5, 1, 0)
predPerc.logModel.test <- predict(logModel, newdata = test, type = 'response')
head(predPerc.logModel.test)
pred.logModel.test <- ifelse(predPerc.logModel.test > 0.5, 1, 0)
table(pred.logModel.test, test$Happy)
predPerc.logModel.test <- predict(logModel, newdata = test, type = 'response')
pred.logModel.test <- ifelse(predPerc.logModel.test > 0.5, 1, 0)
table(pred.logModel.test, test$Happy)
library(rpart)  # for rpart()
library(rpart.plot)  # for prp()
CARTmodel <- rpart(Happy ~ ., data = train, method = 'class')
prp(CARTmodel, varlen=0)  # varlen=0 instructs prp() to use full variable names
predPerc.CARTmodel.test <- predict(CARTmodel, newdata = test)[ , 2]
pred.CARTmodel.test <- ifelse(predPerc.CARTmodel.test > 0.45, 1, 0)
table(pred.CARTmodel.test, test$Happy)
library(randomForest)
train$Happy <- as.factor(train$Happy)
test$Happy <- as.factor(test$Happy)
set.seed(123)
RFmodel <- randomForest(Happy ~ ., data = train, ntrees=200)
predPerc.RFmodel.test <- predict(RFmodel, newdata = test, type = 'prob')[ , 2]
pred.RFmodel.test <- ifelse(predPerc.RFmodel.test > 0.5, 1, 0)
table(pred.RFmodel.test, test$Happy)
library(caret)  # for cross-validation
library(e1071)  # for cross-validation
# setting the training parameters and finding the ideal complex parameter (cp)
trControl <- trainControl(method = 'cv', number = 10)
tuneGrid <- expand.grid(.cp = (1:50) * 0.01)
set.seed(123)
train(Happy ~ .,
data = train,
method = 'rpart',
trControl = trControl,
tuneGrid = tuneGrid)
# creating a 10-fold cross-validated CART model
cv10CARTmodel <- rpart(Happy ~ ., data = train, cp = 0.01)
predPerc.cv10CARTmodel.test <- predict(cv10CARTmodel, newdata = test)[ , 2]
pred.cv10CARTmodel.test <- ifelse(predPerc.cv10CARTmodel.test > 0.45, 1, 0)
table(pred.cv10CARTmodel.test, test$Happy)
library(MASS)  # for lda()
LDAmodel <- lda(Happy ~ ., data = train)
predPerc.LDAmodel.test <- predict(LDAmodel, newdata = test)$posterior[ , 2]
pred.LDAmodel.test <- ifelse(predPerc.LDAmodel.test > 0.5, 1, 0)
table(pred.LDAmodel.test, test$Happy)
train(Happy ~ . - YOB,
data = train,
method = 'rpart',
trControl = trControl,
tuneGrid = tuneGrid)
set.seed(123)
train(Happy ~ . - YOB,
data = train,
method = 'rpart',
trControl = trControl,
tuneGrid = tuneGrid)
logModel <- glm(Happy ~ ., data = train, family = binomial)
predPerc.logModel.test <- predict(logModel, newdata = test, type = 'response')
pred.logModel.test <- ifelse(predPerc.logModel.test > 0.5, 1, 0)
table(pred.logModel.test, test$Happy)
set.seed(123)
RFmodel <- randomForest(Happy ~ . - YOB, data = train, ntrees=200)
predPerc.RFmodel.test <- predict(RFmodel, newdata = test, type = 'prob')[ , 2]
pred.RFmodel.test <- ifelse(predPerc.RFmodel.test > 0.5, 1, 0)
table(pred.RFmodel.test, test$Happy)
set.seed(123)
RFmodel <- randomForest(Happy ~ ., data = train, ntrees=200)
predPerc.RFmodel.test <- predict(RFmodel, newdata = test, type = 'prob')[ , 2]
pred.RFmodel.test <- ifelse(predPerc.RFmodel.test > 0.5, 1, 0)
table(pred.RFmodel.test, test$Happy)
train$Happy <- as.factor(train$Happy)
test$Happy <- as.factor(test$Happy)
set.seed(123)
RFmodel <- randomForest(Happy ~ ., data = train, ntrees=200)
pred.RFmodel.train <- ifelse(predPerc.RFmodel.train > 0.5, 1, 0)
predPerc.RFmodel.test <- predict(RFmodel, newdata = test, type = 'prob')[ , 2]
pred.RFmodel.test <- ifelse(predPerc.RFmodel.test > 0.5, 1, 0)
table(pred.RFmodel.test, test$Happy)
rm(list = ls())
train_test <- read.csv('data/imputed/train_test_imputed.csv')
final_test <- read.csv('data/imputed/final_test_imputed.csv')
table(train$Happy)
set.seed(123)
split <- sample.split(train_test$Happy, SplitRatio=0.75)
train <- train_test[split==TRUE, ]
test <- train_test[split==FALSE, ]
table(test$Happy)
rm(list=ls())
getwd()
setwd('/Users/hawooksong/Desktop/kaggle_show_of_hands/approach1')
initial_train_test.data.file <- 'data/raw/initial_train_test.csv'
final_test.data.file <- 'data/raw/final_test.csv'
missing.types <- c("NA", "")
train_test.column.types <- c('integer',   # UserID
'integer',   # YOB
'factor',    # Gender
'factor',    # Income
'factor',    # HouseholdStatus
'factor',    # EducationLevel
'factor',    # Party
'integer',   # Happy
rep('factor', 101),    # questions
'integer')       # votes
final_test.column.types <- train_test.column.types[-8]
head(train_test.original)
initial_train_test.original <- read.csv(initial_train_test.data.file,
colClasses=train_test.column.types,
na.strings=missing.types)
final_test.original <- read.csv(final_test.data.file,
colClasses=final_test.column.types,
na.strings=missing.types)
dim(train_test.original); dim(final_test.original)
dim(initial_train_test.original); dim(final_test.original)
head(train_test.original)
head(initial_train_test.original)
str(train_test.original)
initial_train_test.original$Income <- factor(initial_train_test.original$Income,
levels = c('under $25,000',
'$25,001 - $50,000',
'$50,000 - $74,999',
'$75,000 - $100,000',
'$100,001 - $150,000',
'over $150,000'))
initial_train_test.original$HouseholdStatus <- factor(initial_train_test.original$HouseholdStatus,
levels = c('Single (no kids)',
'Single (w/kids)',
'Domestic Partners (no kids)',
'Domestic Partners (w/kids)',
'Married (no kids)',
'Married (w/kids)'))
initial_train_test.original$EducationLevel <- factor(initial_train_test.original$EducationLevel,
levels = c('Current K-12',
'High School Diploma',
'Associate\'s Degree',
'Current Undergraduate',
'Bachelor\'s Degree',
'Master\'s Degree',
'Doctoral Degree'))
rm(list=ls())
initial_train_test.data.file <- 'data/raw/initial_train_test.csv'
final_test.data.file <- 'data/raw/final_test.csv'
missing.types <- c("NA", "")
train_test.column.types <- c('integer',   # UserID
'integer',   # YOB
'factor',    # Gender
'factor',    # Income
'factor',    # HouseholdStatus
'factor',    # EducationLevel
'factor',    # Party
'integer',   # Happy
rep('factor', 101),    # questions
'integer')       # votes
final_test.column.types <- train_test.column.types[-8]
initial_train_test <- read.csv(initial_train_test.data.file,
colClasses=train_test.column.types,
na.strings=missing.types)
final_test <- read.csv(final_test.data.file,
colClasses=final_test.column.types,
na.strings=missing.types)
dim(initial_train_test); dim(final_test)
head(initial_train_test)
str(initial_train_test)
summary(initial_train_test)
initial_train_test$Income <- factor(initial_train_test$Income,
levels = c('under $25,000',
'$25,001 - $50,000',
'$50,000 - $74,999',
'$75,000 - $100,000',
'$100,001 - $150,000',
'over $150,000'))
initial_train_test$HouseholdStatus <- factor(initial_train_test$HouseholdStatus,
levels = c('Single (no kids)',
'Single (w/kids)',
'Domestic Partners (no kids)',
'Domestic Partners (w/kids)',
'Married (no kids)',
'Married (w/kids)'))
initial_train_test$Age <- 2014 - initial_train_test$YOB
final_test$Age <- 2014 - final_test$YOB
class(initial_train_test$Age)
class(final_test$Age)
initial_train_test$UserID <- NULL
library(ggplot2)
ggplot(initial_train_test) +
geom_histogram(aes(x=YOB, fill=factor(Happy)),
position='fill',
binwidth=1)
head(sort(initial_train_test$Age), 40)
tail(sort(initial_train_test$Age), 40)
initial_train_test.invalid <- subset(initial_train_test, Age < 13 | Age > 100)
final_test.invalid <- subset(final_test, Age < 13 | Age > 100)
initial_train_test.invalid$Happy
initial_train_test <- subset(initial_train_test, Age > 13 & Age < 100)
final_test <- subset(final_test, Age > 13 & Age < 100)
set.seed(123)
initial_train_test.dep <- subset(initial_train_test, select=Happy)
